Developed project :
    anaconda3 python 3.7
    win 10
    
       

install:
    on windows :
        install winutils
        set as global env %HADOOP_HOME% = "C:\\winutils\\hadoop-X.X.X"   # if not spark will use default log4j profile 
        
    conda create --name elementor --file requirements.txt            #create virtual env and install requrements an interpreter
    conda activate elementor                                         #actovate virtual env
    python ./main.py                                                 #execute program
    
    

About project:
I create txt file data/StructuredStreamingText.txt with static data simulated stream events.

The flow is divided into two parts:

    1) First uploading data in  (staging) dataframe and split it for into parts
        * Invalid record - ( for example invalied user_id iec...) log in out/errorlog
        * Valid  record  - will be used as base dataframe (analytics) for the data flow

    2) Adding fields on dataframe (analytics)
       * Adding sessionid over start_session event and user_id
       * Adding pattern (question 4) event to dataframe

    Then execute quries (questions) over prepared dataframe
    out/result

notes:
    question 2:
       * used WITH CLAUSE for combine timstamp start_session event to conversion event
    question 3:
       * refered only to  in_page events only till last conversion per user_id
    question 4:
       * added amount per user that passed through patterns list
* ./tests.py  unittest
